"""
Phase 2: AI ì´ë¯¸ì§€/ì˜ìƒ ìƒì„± í†µí•© ì„œë¹„ìŠ¤

ì „ì²´ íŒŒì´í”„ë¼ì¸:
1. ì‚¬ì§„ ê²€ì¦ ë° ì •ë©´ ë³€í™˜
2. ì œí’ˆ í•©ì„±
3. ê³ ì–‘ì´ ë†€ì´ ì¥ë©´ í•©ì„±
4. 5ì´ˆ ë™ì˜ìƒ ìƒì„±
"""

import asyncio
import logging
from typing import Dict, List, Optional
from pathlib import Path

from services.comfyui_client import comfyui_client
try:
    from services.qwen_image_edit import QwenImageEditor
    QWEN_AVAILABLE = True
except Exception as e:
    logging.warning(f"Qwen2-VL not available: {e}")
    QWEN_AVAILABLE = False

from services.image_composer import image_composer
from chatbot.ollama_client import ollama_client

logger = logging.getLogger(__name__)


class AIGenerationService:
    """AI ì´ë¯¸ì§€/ì˜ìƒ ìƒì„± í†µí•© ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.qwen_editor = QwenImageEditor() if QWEN_AVAILABLE else None
        self.use_advanced_pipeline = False  # ComfyUI ì‚¬ìš© ì—¬ë¶€

    async def process_consultation_images(
        self,
        consultation_data: Dict
    ) -> Dict:
        """
        ìƒë‹´ ì •ë³´ë¥¼ ë°›ì•„ ì „ì²´ AI ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            consultation_data: {
                "room_image": "path/to/room.jpg",
                "cat_photos": ["path/to/cat1.jpg", ...],
                "products": [...],
                "expected_activity": "ë†’ì€ ê³³ì—ì„œ í¸ì•ˆí•˜ê²Œ ì‰¬ëŠ” ëª¨ìŠµ",
                "cat_count": 2,
                ...
            }

        Returns:
            {
                "room_front_view": "ì •ë©´ ë³€í™˜ëœ ì‚¬ì§„",
                "product_composition": "ì œí’ˆ í•©ì„± ì´ë¯¸ì§€",
                "cat_composition": "ê³ ì–‘ì´ + ì œí’ˆ ì´ë¯¸ì§€",
                "animation_video": "5ì´ˆ ë™ì˜ìƒ",
                "processing_log": [...],
                "quality_report": {...}
            }
        """
        result = {
            "room_front_view": None,
            "product_composition": None,
            "cat_composition": None,
            "animation_video": None,
            "processing_log": [],
            "quality_report": {}
        }

        try:
            # Step 1: ì‚¬ì§„ ê²€ì¦ ë° ì •ë©´ ë³€í™˜
            room_image = consultation_data.get("room_image")
            if not room_image:
                raise ValueError("Room image is required")

            result["processing_log"].append("Step 1: ì‚¬ì§„ ë¶„ì„ ì‹œì‘...")

            front_view_image = await self._validate_and_convert_image(
                room_image,
                result
            )

            # Step 2: ì œí’ˆ ë°°ì¹˜ ê³„ì‚°
            result["processing_log"].append("Step 2: ìµœì  ì œí’ˆ ë°°ì¹˜ ê³„ì‚°...")

            products = await self._calculate_product_placement(
                consultation_data,
                result
            )

            # Step 3: ì œí’ˆ í•©ì„±
            result["processing_log"].append("Step 3: ì œí’ˆ í•©ì„± ì¤‘...")

            product_composition = await self._compose_products(
                front_view_image,
                products,
                result
            )

            result["product_composition"] = product_composition

            # Step 4: ê³ ì–‘ì´ ì‚¬ì§„ì´ ìˆë‹¤ë©´ í•©ì„±
            cat_photos = consultation_data.get("cat_photos", [])
            if cat_photos and len(cat_photos) > 0:
                result["processing_log"].append("Step 4: ê³ ì–‘ì´ í•©ì„± ì¤‘...")

                cat_composition = await self._compose_cats(
                    product_composition,
                    cat_photos[0],  # ì²« ë²ˆì§¸ ê³ ì–‘ì´ ì‚¬ì§„ ì‚¬ìš©
                    result
                )

                result["cat_composition"] = cat_composition

                # Step 5: ë™ì˜ìƒ ìƒì„± (ê¸°ëŒ€í•˜ëŠ” í™œë™ì´ ìˆë‹¤ë©´)
                expected_activity = consultation_data.get("expected_activity")
                if expected_activity:
                    result["processing_log"].append("Step 5: ë™ì˜ìƒ ìƒì„± ì¤‘...")

                    animation = await self._generate_animation(
                        cat_composition,
                        cat_photos[0],
                        expected_activity,
                        result
                    )

                    result["animation_video"] = animation

            result["processing_log"].append("âœ… ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ!")

        except Exception as e:
            logger.error(f"AI generation failed: {str(e)}")
            result["processing_log"].append(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            result["error"] = str(e)

        return result

    async def _validate_and_convert_image(
        self,
        image_path: str,
        result: Dict
    ) -> str:
        """ì‚¬ì§„ ê²€ì¦ ë° ì •ë©´ ë³€í™˜"""

        # Qwenìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„
        if self.qwen_editor:
            try:
                analysis = await self.qwen_editor.analyze_image_quality(image_path)
                result["quality_report"] = analysis

                result["processing_log"].append(
                    f"ğŸ“Š ì‚¬ì§„ ë¶„ì„ ì™„ë£Œ - í’ˆì§ˆ: {analysis['quality_score']}/100"
                )

                # ì •ë©´ ë³€í™˜ í•„ìš” ì—¬ë¶€ íŒë‹¨
                if analysis.get("recommendation") == "convert_to_front_view":
                    result["processing_log"].append(
                        "ğŸ”„ ì‚¬ì§„ ê°ë„ ë³´ì • ì¤‘... (ì•½ 1ë¶„ ì†Œìš”)"
                    )

                    front_view = await self.qwen_editor.convert_to_front_view(
                        image_path
                    )

                    result["room_front_view"] = front_view
                    return front_view

            except Exception as e:
                logger.warning(f"Qwen analysis failed: {e}")
                result["processing_log"].append(
                    "âš ï¸ ìë™ ë¶„ì„ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ì§„ ì‚¬ìš©"
                )

        # ë¶„ì„ ë¶ˆê°€ëŠ¥ ë˜ëŠ” ë¶ˆí•„ìš”í•œ ê²½ìš° ì›ë³¸ ì‚¬ìš©
        result["room_front_view"] = image_path
        return image_path

    async def _calculate_product_placement(
        self,
        consultation_data: Dict,
        result: Dict
    ) -> List[Dict]:
        """AIë¥¼ ì‚¬ìš©í•œ ìµœì  ì œí’ˆ ë°°ì¹˜ ê³„ì‚°"""

        # Ollamaë¥¼ í†µí•´ ì œí’ˆ êµ¬ì„± ì¶”ì²œ
        analysis = await ollama_client.analyze_consultation_data(
            consultation_data
        )

        result["processing_log"].append(
            f"ğŸ’¡ AI ì¶”ì²œ: {len(analysis.get('recommendations', {}))}ê°€ì§€ ì œí’ˆ êµ¬ì„±"
        )

        # ì¶”ì²œ ì œí’ˆì„ ì‹¤ì œ ë°°ì¹˜ ì •ë³´ë¡œ ë³€í™˜
        products = []

        # ë²½ ìº£ì›Œì»¤
        wall_walkers = analysis.get("recommendations", {}).get("wall_walkers", {})
        wall_count = wall_walkers.get("count", 6)

        for i in range(wall_count):
            products.append({
                "id": "wall_walker_30",
                "type": "wall walker",
                "material": "wooden",
                "quantity": 1
            })

        # ì²œì¥ ìº£ì›Œì»¤
        ceiling_walkers = analysis.get("recommendations", {}).get("ceiling_walkers", {})
        ceiling_count = ceiling_walkers.get("count", 2)

        for i in range(ceiling_count):
            products.append({
                "id": "ceiling_walker_60",
                "type": "ceiling walker",
                "material": "wooden",
                "quantity": 1
            })

        # íœ´ì‹ì²˜
        rest_spots = analysis.get("recommendations", {}).get("rest_spots", {})
        rest_count = rest_spots.get("count", 2)

        for i in range(rest_count):
            products.append({
                "id": "rest_spot_circle",
                "type": "rest spot",
                "material": "wooden",
                "quantity": 1
            })

        return products

    async def _compose_products(
        self,
        room_image: str,
        products: List[Dict],
        result: Dict
    ) -> str:
        """ì œí’ˆ í•©ì„±"""

        if self.use_advanced_pipeline:
            # ComfyUI ì‚¬ìš© (Phase 2 ê³ ê¸‰ ê¸°ëŠ¥)
            try:
                result["processing_log"].append(
                    "ğŸ¨ AI ê³ ê¸‰ í•©ì„± ì‹œì‘ (2-5ë¶„ ì†Œìš”)..."
                )

                composition = await comfyui_client.product_composition(
                    room_image_path=room_image,
                    products=products
                )

                return composition

            except Exception as e:
                logger.warning(f"ComfyUI composition failed: {e}, falling back to simple")
                result["processing_log"].append(
                    "âš ï¸ ê³ ê¸‰ í•©ì„± ì‹¤íŒ¨, ê¸°ë³¸ í•©ì„± ì‚¬ìš©"
                )

        # Phase 1 ê¸°ë³¸ í•©ì„± (PIL/OpenCV)
        result["processing_log"].append(
            "ğŸ–¼ï¸ ê¸°ë³¸ ì´ë¯¸ì§€ í•©ì„± (ë¹ ë¥¸ ëª¨ë“œ)..."
        )

        # ìë™ ë°°ì¹˜ ê³„ì‚°
        space_dimensions = {
            "width": 300,  # ì‹¤ì œë¡œëŠ” consultation_dataì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
            "height": 250,
            "ceiling_height": 240
        }

        placed_products = image_composer.auto_place_products(
            space_dimensions=space_dimensions,
            products_config=products,
            cat_count=2  # ì‹¤ì œ ê°’ ì‚¬ìš©
        )

        composition = image_composer.composite_simple(
            background_path=room_image,
            products=placed_products,
            output_path=f"static/composites/composition_{Path(room_image).stem}.jpg"
        )

        return composition

    async def _compose_cats(
        self,
        base_image: str,
        cat_photo: str,
        result: Dict
    ) -> str:
        """ê³ ì–‘ì´ í•©ì„±"""

        if self.use_advanced_pipeline:
            # ComfyUI + IP-Adapter ì‚¬ìš©
            try:
                result["processing_log"].append(
                    "ğŸ± ê³ ì–‘ì´ AI í•©ì„± ì¤‘ (1-3ë¶„ ì†Œìš”)..."
                )

                # TODO: IP-Adapterë¥¼ ì‚¬ìš©í•œ ê³ ì–‘ì´ ì–¼êµ´ ìœ ì§€ í•©ì„±
                # í˜„ì¬ëŠ” ê¸°ë³¸ í•©ì„±ìœ¼ë¡œ í´ë°±

                pass

            except Exception as e:
                logger.warning(f"Cat composition failed: {e}")

        # ê¸°ë³¸ í•©ì„±
        result["processing_log"].append(
            "ğŸ± ê³ ì–‘ì´ ê¸°ë³¸ í•©ì„±..."
        )

        # ë‹¨ìˆœ ì˜¤ë²„ë ˆì´ (Phase 1)
        # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ í•©ì„± í•„ìš”
        return base_image

    async def _generate_animation(
        self,
        base_image: str,
        cat_photo: str,
        activity_prompt: str,
        result: Dict
    ) -> str:
        """5ì´ˆ ë™ì˜ìƒ ìƒì„±"""

        if not self.use_advanced_pipeline:
            result["processing_log"].append(
                "âš ï¸ ë™ì˜ìƒ ìƒì„±ì€ ê³ ê¸‰ ëª¨ë“œì—ì„œë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤"
            )
            return None

        try:
            result["processing_log"].append(
                "ğŸ¬ ë™ì˜ìƒ ìƒì„± ì¤‘ (3-10ë¶„ ì†Œìš”)..."
            )

            animation = await comfyui_client.cat_animation(
                base_image_path=base_image,
                cat_image_path=cat_photo,
                activity_prompt=activity_prompt
            )

            result["processing_log"].append(
                "âœ… 5ì´ˆ ë™ì˜ìƒ ìƒì„± ì™„ë£Œ!"
            )

            return animation

        except Exception as e:
            logger.error(f"Animation generation failed: {e}")
            result["processing_log"].append(
                f"âŒ ë™ì˜ìƒ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            )
            return None

    def enable_advanced_pipeline(self, enable: bool = True):
        """
        ê³ ê¸‰ íŒŒì´í”„ë¼ì¸ í™œì„±í™”/ë¹„í™œì„±í™”

        Args:
            enable: True = ComfyUI ì‚¬ìš©, False = ê¸°ë³¸ í•©ì„±ë§Œ ì‚¬ìš©
        """
        self.use_advanced_pipeline = enable
        logger.info(
            f"Advanced pipeline {'enabled' if enable else 'disabled'}"
        )


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ai_generation_service = AIGenerationService()
